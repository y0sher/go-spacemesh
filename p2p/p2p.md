# P2P Design

`p2p` package should expose minimal functions as a service to other components in Spacemesh.
- `SendMessage(MSG, nodeID)`
- `Broadcast(MSG)`
- `OnRecv(msgtype, optional:peer, callback(func or chan))`  / `RegisterProtocol(Protocol, handler)`

## Node

The `LocalNode` is the struct that represent a node in our network. in order to use the above P2P functionality a `LocalNode` must be created from within the `p2p` package.
It holds a Private and Public key and an Identity
`LocalNode` can initialize from scratch or form existing persistent keys and files. it needs a local address and port.

To enable this it implements multiple components:

## Swarm
`Swarm` manages the communications with peers while maintaining a list of healthy connected peers and an updated `RoutingTable` of nodes to choose from.
`Swarm`'s responsibility is to receive and send network messages and connections using events propagated from `Net`, handle them - check that they are in a valid format and time and pass them over to the right protocol handler using `Demuxer`.

*TODO*: Describe the patterns used in `Swarm` the event loop, state management and how to prevent deadlocks.
Also about the bootstrap loop and the network loop.

### P2P Protocols :
  - `HandshakeProtocol` - Used to initiate a secured connection with peers
  ```protobuf
  // The initiator creates a HandshakeData object
  // with a random IV based on the requested node Public Key
  // and our own Private Key.
  // The request also contains basic details
  // about the initiator to decide if we can can create a session
  message HandshakeData {
      bytes sessionId = 1;    // for req - same as iv. for response - set to req id
      bytes payload = 2; // empty for handshake
      int64 timestamp = 3; // sending time
      string clientVersion = 4; // client version of the sender
      int32 networkID = 5; // network id of sending node
      string protocol = 6; // 'handshake/req' || 'handshake/resp'
      bytes nodePubKey = 7; // 65 bytes uncompressed
      bytes iv = 8; // 16 bytes - AES-256-CBC IV
      bytes pubKey = 9; // 65 bytes (uncompressed) ephemeral public key
      bytes hmac = 10; // HMAC-SHA-256 32 bytes
      string tcpAddress = 11; // ipv4 tcp address and port e.g. x.x.x.x:2424 that the remote node is accepting connections on
      string sign = 12; // hex encoded string 32 bytes sign of all above data by node public key (verifies he has the priv key and he wrote the data
  }
  ```
  - `FindNodeProtocol`  - The only `Kademlia` protocol we implement, used for bootstrapping, discovery and probing of nodes.
```protobuf
// FindNode is a Kademlia operation where the initiator sends a dht.ID
// To another node and gets the list of closest nodes from the requested node.
// If the original node isn't found by one request it keeps
// requesting from the list until found or no new results
message FindNodeReq {
      Metadata metadata = 1;
      bytes nodeId =2;
      int32 maxResults = 3;
}
message FindNodeResp {
      Metadata metadata = 1;
      repeated NodeInfo nodeInfos = 2;
}
```


  ### `Demuxer`
  The `Demuxer` is a junction of protocols and handlers, it enables `Swarm` to take care only of validation of a message and then route to the registered protocols through `Demuxer`.

  `Demuxer` exposes 2 main functions for registering and routing protocol messages.

  - `RegisterProtocolHandler(handler ProtocolRegistration)` - Let's new protocols register themselves.
  - `RouteIncomingMessage(msg IncomingMessage)` - Routes the message to the appropriate handler


  ### Protocols
  All protocols include `Metadata` about the author or sender of the message.

```protobuf
message Metadata {
    string protocol = 1;      // Protocol id string
    bytes reqId = 2;          // Unique request id. Generated by caller. Returned in responses.
    string clientVersion = 3; // Author client version
    int64 timestamp = 4;      // Unix time - authoring time (not sending time)
    bool gossip = 5;          // True to have receiver peer gossip the message to its neighbors
    bytes authPubKey = 6;     // Authoring node Secp256k1 public key (32bytes) - may not be sender
    string authorSign = 7;    // Signature of message data by author + method specific data by message creator node. format: hexEncode([]bytes)
}
```

  Currently the only external protocol we implement over the p2p stack is `Ping`

  ```protobuf
  // ping is our example protocol
  message PingReqData {
      Metadata metadata = 1;
      string ping = 2; // the echo message itself - protocol specific - starts at #2
  }

  message PingRespData {
      Metadata metadata = 1;
      string pong = 2; // the echo message itself - protocol specific - starts at #2
  }
  ```

  *TODO*: describe our process of reading protocol messages metadata for validation without reading the protocol payload


## Components used by `Swarm`

## `net` Packge -  `Net` and `Connection` (with `delimited`)
  `Net` is basically a connection manager. it is used to initiate and to accept all network operations including connections and messages.

  `Net` exposes `DialTCP(ip, timeout, keepalive)` (** to be `Dial` to enable more transport flexibility **)  which `Swarm` uses to initiate connections.
  When `Net` is constructed it starts a loop that listens to incoming connections.

  Each connection that established with a `Connection` struct that holds a go `net.Conn` TCP connection that is getting piped through `delimited`.
  `delimited` wraps the connections and reads/writes length delimited messages over it.

`Net` holds channels that are used to pass messages and connections through to `Swarm` to handle them. they are exposed by the following methods :

- 	`GetNewConnections()` - A channel of connections
-	`GetClosingConnections()` - A channel of connections
-	`GetConnectionErrors()` - A channel of `ConnectionError`
-	`GetIncomingMessage()` - A channel of `IncomingMessage`s
-	`GetMessageSendErrors()` A channel of `MessageSendError`
-	`GetMessageSentCallback()` A channel `MessageSentEvent` to affirm messages sent (*NOTE*: not fully implemented yet)

Messages are received at the `delimited` wrapped `Connection` and sent to `net.GetIncomingMessage()` through through the `Connection` then get caught and treated in `Swarm`, which means validation of session, signatures and time. and after that muxing to the right protocol.

### Wire message format

We are using our own simple length-prefix binary format:
```
<32 bits big-endian data-length><message binary data (protobufs-bin-encoded)>
```
We need to use length prefixed protobufs messages becuase protobufs data doesn't include length and we need this to allow multiple messages on the same tcp/ip connection . see p2p2.conn.go.

## Package `dht`

`dht` is an integral part of the `p2p` package, it is used to implement the `Kademlia` DHT. A [DHT](https://en.wikipedia.org/wiki/Distributed_hash_table) is mainly used in p2p networks to lookup and probe an ID into an IP address or a file hash to the node it's stored on.

we broke `Kademlia` into 2 parts the node store known as the `RoutingTable` and the protocols, we skip most of `Kademlia` protocol and implement only the `FindNode` protocol which is used to probe node's in the network by IDs.

The `RoutingTable` is a table that holds all the nodes that we know from communications or lookups in the network, it is in charge of keeping that list of nodes healthy and ready for when the node needs to preform a lookup or get information from the network.

For `Kademlia` lookups we're hashing the original nodeIDs to SHA256 hashes. every node ID has its deterministic `dht.ID`.

Using those hashes we calculate and arrange nodes inside the `RoutingTable` according to their "Distance" from us in `Kademlia` "Distance" is determined by a space called the `XOR` Keyspace

### The `XOR` Keyspace

The `dht.ID`s exist inside the `XOR` keyspace, `dht` also exposes sorting and functionality in this space which should help determine the distance to other nodes in this space.

"Distance" to a node in the XOR space is the result of XORing the IDs of both nodes.
XOR is symmetric and will always produce the same result for the same set of IDs.

When XORing the `dht.ID`s together we use the given result and check the amount of leading zero's in the binary representation of the result.
this is called the `Common Prefix Length` of thses two `dht.ID`s.

The higher the `Common Prefix Length` the closer the node to us.

Example:
```
// For this example and readability we assume the keyspace is 24bits (3 bytes) long instead of 256bits
"123" as dht.ID = a665a4
binary - 10100110110010110100100

"567" as dht.ID = 97a6d2
binary - 100101111010011011010010

XORing : {
"123"(a665a4) XOR "567"(97a6d2) = 31c376
binary - 110001110000111110110
}

The zero prefix length of the xor result
which is the Common Prefix Length : 2
(100101111010011011010010)

A third ID : "789"
"789" as dht.ID = 35a9e3
binary - 1101011010100111100011
XORing "123" with "789" {
  "123"(a665a4) XOR "789"(35a9e3) = 93cc47
  binary - 110001110000111110110
}

The zero prefix length of the xor result
which is the Common Prefix Length : 0

567 is closer in xor space to 123 than 789

```


### `RoutingTable`

`dht` implements `RoutingTable` which is a table that holds all the nodes that we know from communications or lookups in the network.

The `RoutingTable` is in charge of keeping that list of nodes healthy and ready for when the node needs to preform a lookup or get information from the network.

The nodes in the `RoutingTable` are arranged inside `Bucket`s in `Kademlia` they are called `KBuckets`. every bucket represent a `Common Prefix Length` we store 20 `Buckets` which hold nodes according to our `BucketSize` parameter which is currently 20.
The higher the `Bucket` number, the closer the nodes in the list to us.
Most of the nodes are going to be in `Bucket` 0, half of the network according to `Kademlia`, half of the rest of the nodes will be in `Bucket` 1, and so forth.
this is why we hold only 20 buckets.

Our routing table operations implement the `Kademlia` protocol operations
- `Update(peer) = if we know this peer we move it to the top of the `Bucket`
 If we never heard of this peer, we insert it to the appropriate bucket, if this bucket is full then we ping the last contacted peer (which should be the last in the bucket) and see if its alive. we then compare latency metrics and choose the best of the two.
- `NearestPeers(dht.ID)` - We check in the appropriate routing table and return the closest peers that we have to this node.
- `FindPeer(dht.ID)` - return the contact information of this peer if we have it and null if we don't.

## Bootstrapping

The bootstrap process is what lets us join the p2p network.

When the Spacemesh node start it has to fill it's `RoutingTable` with fresh and active nodes, to do this process the node is equipped with a list of Bootstrap Nodes. every Spacemesh node can act as Bootstrap Node so they can be replaced with a detailed acquired from any other source.

To bootstrapping process is just a matter of issuing the `FindNode` `Kademlia` protocol (refer to `P2P Protocols`) with the node own `dht.ID` as a parameter. this will return a list of nodes close to our own `dht.ID`, we'll query those
nodes (Concurrently) for the same ID until we don't get new results and we queried all nodes.

This will make every node that see our request to update its `RoutingTable` with our node's `dht.ID` and also fill our `dht.ID` with these nodes.
